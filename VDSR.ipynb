{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qpQsnXeN9RWpyvoCaTtxU83Vb1-oDBSw","authorship_tag":"ABX9TyMn0MBIc9FJMVRUVy/cs5Ix"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"68QUryp0Ef8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#unzipping train dataset\n","!mkdir data\n","!unzip /content/drive/MyDrive/data291.zip -d /content/data/\n","!mkdir data/datalr"],"metadata":{"id":"9WKJ5v8m6uT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#unzipping test dataset\n","!unzip /content/drive/MyDrive/set5.zip -d /content/data/"],"metadata":{"id":"V0itt4e1Vs93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/set14.zip -d /content/data"],"metadata":{"id":"4p69fkIohZb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir data/model\n","!mkdir data/output"],"metadata":{"id":"5mHG8R93odEe","executionInfo":{"status":"ok","timestamp":1678168425943,"user_tz":-540,"elapsed":341,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","\n","#resize set5\n","\n","\n","image_link = \"/content/data/set5\"\n","\n","\n","for j in range(2,5):\n","  scale = j\n","  for i in range (1,6):\n","      image_file = image_link +\"/s\" +str(i)+ \".png\"\n","      image = pil_image.open(image_file)\n","      image_width = image.width\n","      image_height = image.height\n","      image = image.resize((image.width//scale, image.height//scale))\n","      image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n","      image.save( image_link +\"/s\"+ str(i)+\"lr\" + str(scale) + \".png\")"],"metadata":{"id":"giDBM2_VnglQ","executionInfo":{"status":"ok","timestamp":1678168289091,"user_tz":-540,"elapsed":1189,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","\n","#resize set14\n","\n","\n","image_link = \"/content/data/set14\"\n","\n","\n","for j in range(2,5):\n","  scale = j\n","  for i in range (1,15):\n","      image_file = image_link +\"/ss\" +str(i)+ \".png\"\n","      image = pil_image.open(image_file)\n","      image_width = image.width\n","      image_height = image.height\n","      image = image.resize((image.width//scale, image.height//scale))\n","      image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n","      image.save( image_link +\"/ss\"+ str(i)+\"lr\" + str(scale) + \".png\")"],"metadata":{"id":"kZkJXQr1ngV0","executionInfo":{"status":"ok","timestamp":1678168309028,"user_tz":-540,"elapsed":3889,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","\n","#resize image to create Low Resolution image\n","\n","\n","image_link = \"/content/data/data\"\n","\n","for j in range(2,5):\n","  scale = j\n","  for i in range (1,292):\n","      image_file = image_link +\"/t\" +str(i)+ \".png\"\n","      image = pil_image.open(image_file)\n","      image_width = image.width\n","      image_height = image.height\n","      image = image.resize((image.width//scale, image.height//scale))\n","      image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n","      image.save( image_link +\"lr/t\"+ str(i)+\"lr\" + str(scale) + \".png\")"],"metadata":{"id":"4LDpG-eeJjmt","executionInfo":{"status":"ok","timestamp":1678168372305,"user_tz":-540,"elapsed":54024,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.optim as optim\n","from torch.utils.data.dataloader import DataLoader\n","from PIL import Image\n","from numpy import asarray\n","import PIL.Image as pil_image\n","from torchvision import transforms\n","import random\n","import PIL\n","\n","\n","def psnr_between_rgb(img1,img2):\n","    if len(img1.shape) == 4:\n","        img1 = img1.squeeze(0)\n","        img2 = img2.squeeze(0)\n","    y1 = 16. + (64.738 * img1[0, :, :] + 129.057 * img1[1, :, :] + 25.064 * img1[2, :, :]) / 256.\n","    y2 = 16. + (64.738 * img2[0, :, :] + 129.057 * img2[1, :, :] + 25.064 * img2[2, :, :]) / 256.\n","    psnr = (10. * torch.log10(1. / torch.mean((y1 - y2) ** 2)))\n","    return psnr\n","\n","\n","### transform\n","transform_train = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.ToTensor()\n","\n","])\n","\n","####################################################################################\n","#########  MODE SELECTION\n","\n","###    Train with new model    = 1\n","### Train with existing model  = 2\n","###      Test with set5        = 3\n","###     Test with set14        = 4\n","\n","mode = 3\n","\n","transform = True\n","\n","\n","##########  MODEL\n","model_parameters_path = \"/content/data/model/model.pth\"\n","\n","\n","######### PARAMETER\n","learning_rate = 0.01\n","training_epochs = 20\n","batch_size = 16\n","userseed = 123\n","\n","######### FILE ROUTES\n","\n","\n","######### MODEL PARAMETER\n","channel_1 = 64\n","channel_2 = 64\n","channel_3 = 64\n","\n","kernel_1 = 3\n","kernel_2 = 3  \n","kernel_3 = 3\n","\n","####### GRADIENT CLIPPING\n","max_norm = 0.0000000005\n","\n","####### test parameter\n","test_scale = 3\n","\n","\n","#####################################################################################\n","\n","\n","\n","\n","\n","\n","lrarr2 = []\n","lrarr3 = []\n","lrarr4 = []\n","hrarr = []\n","image_link = \"/content/data/data\"\n","sub_image_size = 41\n","stride = 41\n","for i in range (1,292): # total number of image + 1\n","\n","\n","    ### image file opening\n","    lr2image = pil_image.open(image_link + \"lr/t\" + str(i) + \"lr2.png\")\n","    lr3image = pil_image.open(image_link + \"lr/t\" + str(i) + \"lr3.png\")\n","    lr4image = pil_image.open(image_link + \"lr/t\" + str(i) + \"lr4.png\")\n","    hrimage = pil_image.open(image_link + \"/t\" + str(i) + \".png\")\n","\n","\n","    for j in range(0, ( lr2image.size[0]-sub_image_size)//stride +1):\n","        for k in range(0 , (lr2image.size[1]-sub_image_size)//stride +1):\n","                \n","            ### crop image\n","            lr2cropped = lr2image.crop((j*stride, k*stride, j*stride+sub_image_size, k*stride+sub_image_size))\n","            lr3cropped = lr3image.crop((j*stride, k*stride, j*stride+sub_image_size, k*stride+sub_image_size))\n","            lr4cropped = lr4image.crop((j*stride, k*stride, j*stride+sub_image_size, k*stride+sub_image_size)) \n","            hrcropped = hrimage.crop((j*stride, k*stride, j*stride+sub_image_size, k*stride+sub_image_size))\n","\n","            ### post processing of image file\n","            lr2croppedimage = np.array(lr2cropped)\n","            lr3croppedimage = np.array(lr3cropped)\n","            lr4croppedimage = np.array(lr4cropped)  \n","            hrcroppedimage = np.array(hrcropped)    \n","                \n","            lrarr2.append(lr2croppedimage)\n","            lrarr3.append(lr3croppedimage)\n","            lrarr4.append(lr4croppedimage)\n","            hrarr.append(hrcroppedimage)\n","\n","    for j in range(0, ( lr2image.size[0]-sub_image_size)//stride +1):\n","      lr2cropped = lr2image.crop((j*stride, lr2image.size[1]-sub_image_size, j*stride+sub_image_size, lr2image.size[1]))\n","      lr3cropped = lr3image.crop((j*stride, lr2image.size[1]-sub_image_size, j*stride+sub_image_size, lr2image.size[1]))\n","      lr4cropped = lr4image.crop((j*stride, lr2image.size[1]-sub_image_size, j*stride+sub_image_size, lr2image.size[1]))\n","      hrcropped = hrimage.crop((j*stride, lr2image.size[1]-sub_image_size, j*stride+sub_image_size, lr2image.size[1]))\n","\n","      lr2croppedimage = np.array(lr2cropped)\n","      lr3croppedimage = np.array(lr3cropped)\n","      lr4croppedimage = np.array(lr4cropped)\n","      hrcroppedimage = np.array(hrcropped) \n","\n","      lrarr2.append(lr2croppedimage)\n","      lrarr3.append(lr3croppedimage)\n","      lrarr4.append(lr4croppedimage)\n","      hrarr.append(hrcroppedimage)\n"," \n","\n","    for k in range(0 , (lr2image.size[1]-sub_image_size)//stride +1):\n","      lr2cropped = lr2image.crop((lr2image.size[0]-sub_image_size, k*stride, lr2image.size[0], k*stride+sub_image_size))\n","      lr3cropped = lr3image.crop((lr2image.size[0]-sub_image_size, k*stride, lr2image.size[0], k*stride+sub_image_size))\n","      lr4cropped = lr4image.crop((lr2image.size[0]-sub_image_size, k*stride, lr2image.size[0], k*stride+sub_image_size))\n","      hrcropped = hrimage.crop((lr2image.size[0]-sub_image_size, k*stride, lr2image.size[0], k*stride+sub_image_size))\n","      \n","      lr2croppedimage = np.array(lr2cropped)\n","      lr3croppedimage = np.array(lr3cropped)\n","      lr4croppedimage = np.array(lr4cropped)\n","      hrcroppedimage = np.array(hrcropped) \n","\n","      lrarr2.append(lr2croppedimage)\n","      lrarr3.append(lr3croppedimage)\n","      lrarr4.append(lr4croppedimage)\n","      hrarr.append(hrcroppedimage)\n","    \n","    lr2cropped = lr2image.crop((lr2image.size[0]-sub_image_size, lr2image.size[1]-sub_image_size, lr2image.size[0], lr2image.size[1]))\n","    lr3cropped = lr3image.crop((lr2image.size[0]-sub_image_size, lr2image.size[1]-sub_image_size, lr2image.size[0], lr2image.size[1]))\n","    lr4cropped = lr4image.crop((lr2image.size[0]-sub_image_size, lr2image.size[1]-sub_image_size, lr2image.size[0], lr2image.size[1]))\n","    hrcropped = hrimage.crop((lr2image.size[0]-sub_image_size, lr2image.size[1]-sub_image_size, lr2image.size[0], lr2image.size[1]))\n","\n","    lr2croppedimage = np.array(lr2cropped)\n","    lr3croppedimage = np.array(lr3cropped)\n","    lr4croppedimage = np.array(lr4cropped)\n","    hrcroppedimage = np.array(hrcropped) \n","\n","    lrarr2.append(lr2croppedimage)\n","    lrarr3.append(lr3croppedimage)\n","    lrarr4.append(lr4croppedimage)\n","    hrarr.append(hrcroppedimage)\n","\n","\n","class VDSR(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","\n","        self.layer1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(3, channel_1, kernel_size=kernel_1, stride=1, padding = kernel_1//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer1[0].weight)\n","        \n","\n","        self.layer2 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer2[0].weight)\n","\n","        self.layer3 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer3[0].weight)\n","\n","        self.layer4 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer4[0].weight)\n","\n","        self.layer5 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer5[0].weight)\n","\n","        self.layer6 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer6[0].weight)\n","\n","        self.layer7 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer7[0].weight)\n","\n","        self.layer8 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer8[0].weight)\n","        \n","        self.layer9 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer9[0].weight)\n","\n","        self.layer10 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer10[0].weight)\n","\n","        self.layer11 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer11[0].weight)\n","\n","        self.layer12 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer12[0].weight)\n","\n","        self.layer13 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer13[0].weight)\n","\n","        self.layer14 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer14[0].weight)\n","\n","        self.layer15 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer15[0].weight)\n","\n","        self.layer16 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer16[0].weight)\n","\n","        self.layer17 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer17[0].weight)\n","\n","        self.layer18 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer18[0].weight)\n","\n","        self.layer19 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_1, channel_2, kernel_size=kernel_2, stride=1, padding = kernel_2//2),\n","            torch.nn.ReLU(), \n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer19[0].weight)\n","      \n","\n","        self.layer20 = torch.nn.Sequential(\n","            torch.nn.Conv2d(channel_2, 3, kernel_size=kernel_3, stride=1, padding = kernel_3//2),\n","        )\n","        torch.nn.init.kaiming_uniform_(self.layer20[0].weight)\n","        \n","\n","    def forward(self,x):\n","        residual = x\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.layer9(out)\n","        out = self.layer10(out)\n","        out = self.layer11(out)\n","        out = self.layer12(out)\n","        out = self.layer13(out)\n","        out = self.layer14(out)\n","        out = self.layer15(out)\n","        out = self.layer16(out)\n","        out = self.layer17(out)\n","        out = self.layer18(out)\n","        out = self.layer19(out)\n","        out = self.layer20(out)\n","        out = torch.add(out,residual)\n","        return out\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","torch.manual_seed(userseed)\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(userseed)\n","\n","\n","### model load\n","\n","model = VDSR().to(device)\n","\n","if mode ==2 or mode ==3 or mode==4:\n","    model.load_state_dict(torch.load(model_parameters_path))\n","\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","### dataset\n","class TrainDataset(Dataset):\n","    def __init__(self, transform = None):\n","        super(TrainDataset,self).__init__()\n","        self.transform = transform\n","        self.transform_train_1 = transforms.Compose([ # VERTICAL\n","        transforms.ToPILImage(),\n","        transforms.RandomVerticalFlip(p = 1),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_2 = transforms.Compose([ # HORIZONTAL\n","        transforms.ToPILImage(),\n","        transforms.RandomHorizontalFlip(p = 1),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_3 = transforms.Compose([# VERTICAL + HORIZONTAL\n","        transforms.ToPILImage(),\n","        transforms.RandomVerticalFlip(p = 1),\n","        transforms.RandomHorizontalFlip(p = 1),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_4 = transforms.Compose([ # 90 ROTATION\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(degrees = (90,90)),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_5 = transforms.Compose([ #270 ROTATION\n","        transforms.ToPILImage(),\n","        transforms.RandomRotation(degrees = (270,270)),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_6 = transforms.Compose([ #VERTICAL + 90\n","        transforms.ToPILImage(),\n","        transforms.RandomVerticalFlip(p = 1),\n","        transforms.RandomRotation(degrees = (90,90)),\n","        transforms.ToTensor()\n","        ])\n","        self.transform_train_7 = transforms.Compose([ #HORIZONTAL + 90\n","        transforms.ToPILImage(),\n","        transforms.RandomHorizontalFlip(p = 1),\n","        transforms.RandomRotation(degrees = (90,90)),\n","        transforms.ToTensor()\n","        ])\n","\n","    def __getitem__(self,idx):\n","\n","        if self.transform:\n","            # y = random parameter for scale\n","            y = random.random()\n","            if(y<0.334):\n","              lr = lrarr2[idx]\n","            elif(y<0.667):\n","              lr = lrarr3[idx]\n","            else:\n","              lr = lrarr4[idx]\n","            hr = hrarr[idx]\n","            \n","            # x = random parameter for rotation / flip\n","            x = random.random()\n","            \n","            if(x<0.125):\n","                return self.transform_train_1(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_1(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.25):\n","                return self.transform_train_2(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_2(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.375):\n","                return self.transform_train_3(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_3(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.5):\n","                return self.transform_train_4(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_4(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.625):\n","                return self.transform_train_5(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_5(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.75):\n","                return self.transform_train_6(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_6(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            elif(x<0.875):\n","                return self.transform_train_7(lr).to(\"cpu\").numpy().astype(np.float32) ,self.transform_train_7(hrarr[idx]).to(\"cpu\").numpy().astype(np.float32)\n","            else:\n","                return lr.transpose((2,0,1)).astype(np.float32)/255. ,hrarr[idx].transpose((2,0,1)).astype(np.float32)/255.\n","            \n","        return lr.transpose((2,0,1)).astype(np.float32)/255. ,hrarr[idx].transpose((2,0,1)).astype(np.float32)/255. \n","        \n","        \n","    def __len__(self):\n","        return len(hrarr)\n","\n","    \n","\n","if transform:\n","    train_dataset = TrainDataset(transform = transform_train)\n","else:\n","    train_dataset = TrainDataset()\n","\n","### dataloader\n","train_dataloader = DataLoader(dataset = train_dataset,\n","                 batch_size = 16,\n","                 shuffle = True,\n","                 num_workers = 2,\n","                 pin_memory=True,\n","                 drop_last = True)\n","\n","if mode ==1 or mode==2:\n","\n","  for epoch in range(training_epochs):\n","        model.train()\n","        avg_cost=0\n","        sum_psnr1 = 0\n","        count = 0\n","        for data in train_dataloader:\n","            count = count+1\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            \n","            preds = model(inputs)\n","            \n","            \n","            loss = criterion(preds, labels) #MSE\n","            \n","            psnr1 = psnr_between_rgb(inputs,labels)\n","            sum_psnr1 = sum_psnr1+psnr1\n","\n","\n","\n","\n","\n","\n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm/learning_rate)\n","            optimizer.step()\n","            avg_cost += loss / batch_size\n","\n","\n","        print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n","        model.eval()\n","        sum_psnr = 0\n","        \n","\n","        for i in range(1,6):\n","\n","          setlrlr = Image.open(\"/content/data/set5/s\" + str(i) + \"lr\" + str(test_scale)+\".png\")\n","          setlrlr = np.array(asarray(setlrlr)).astype(np.float32)\n","          setlrlr = setlrlr.transpose((2,0,1))\n","          setlrlr /= 255.\n","          setlrlr = torch.from_numpy(setlrlr).to(device)\n","          \n","\n","          sethrhr = Image.open(\"/content/data/set5/s\"+str(i)+\".png\")\n","          sethrhr= np.array(asarray(sethrhr)).astype(np.float32)\n","          sethrhr = sethrhr.transpose((2,0,1))\n","          sethrhr /= 255.\n","          sethrhr = torch.from_numpy(sethrhr).to(device)\n","\n","\n","          with torch.no_grad():\n","              setpreds = model(setlrlr).clamp(0.0,1.0)\n","\n","          psnr = psnr_between_rgb(sethrhr,setpreds)\n","          sum_psnr+=psnr\n","          \n","        print('PSNR: {:.2f}'.format(sum_psnr/5))\n","\n","  torch.save(model.state_dict(), model_parameters_path)\n","\n","### only testing - set5\n","elif mode == 3:\n","  model.eval()\n","  sum_psnr = 0\n"," \n","  for i in range(1,6):\n","    setlrlr = Image.open(\"/content/data/set5/s\" + str(i) + \"lr\" + str(test_scale)+\".png\")\n","    setlrlr = np.array(asarray(setlrlr)).astype(np.float32)\n","    setlrlr = setlrlr.transpose((2,0,1))\n","    setlrlr /= 255.\n","    setlrlr = torch.from_numpy(setlrlr).to(device)\n","          \n","\n","    sethrhr = Image.open(\"/content/data/set5/s\"+str(i)+\".png\")\n","    sethrhr= np.array(asarray(sethrhr)).astype(np.float32)\n","    sethrhr = sethrhr.transpose((2,0,1))\n","    sethrhr /= 255.\n","    sethrhr = torch.from_numpy(sethrhr).to(device)\n","\n","    \n","\n","    with torch.no_grad():\n","        setpreds = model(setlrlr).clamp(0.0,1.0)\n","    \n","    \n","    #save part \n","    '''\n","    preds2 = setpreds.mul(255.0).cpu().numpy()\n","\n","    preds2 = preds2.squeeze()\n","    preds2 = preds2.transpose((1,2,0))\n","    preds2 = np.clip(preds2, 0.0,255.0).astype(np.uint8)\n","    output = pil_image.fromarray(preds2)\n","    output.save(\"/content/data/output/output\"+str(i)+\".png\")\n","    '''\n","    psnr = psnr_between_rgb(sethrhr,setpreds)\n","    sum_psnr+=psnr\n","          \n","  print('PSNR: {:.2f}'.format(sum_psnr/5))\n","\n","### testing set14 \n","elif mode ==4:\n","  model.eval()\n","  sum_psnr = 0\n","   \n","    \n","\n","  for i in range(1,15):\n","    setlrlr = Image.open( \"/content/data/set14/ss\" + str(i) + \"lr\" + str(test_scale)+\".png\")\n","    setlrlr = np.array(asarray(setlrlr)).astype(np.float32)\n","    setlrlr = setlrlr.transpose((2,0,1))\n","    setlrlr /= 255.\n","    setlrlr = torch.from_numpy(setlrlr).to(device)\n","          \n","\n","    sethrhr = Image.open( \"/content/data/set14/ss\" + str(i) + \".png\")\n","    sethrhr= np.array(asarray(sethrhr)).astype(np.float32)\n","    sethrhr = sethrhr.transpose((2,0,1))\n","    sethrhr /= 255.\n","    sethrhr = torch.from_numpy(sethrhr).to(device)\n","\n","\n","    with torch.no_grad():\n","        setpreds = model(setlrlr).clamp(0.0,1.0)\n","\n","    \n","    \n","    \n","    #save part \n","    '''\n","    preds2 = setpreds.mul(255.0).cpu().numpy()\n","\n","    preds2 = preds2.squeeze()\n","    preds2 = preds2.transpose((1,2,0))\n","    preds2 = np.clip(preds2, 0.0,255.0).astype(np.uint8)\n","    output = pil_image.fromarray(preds2)\n","    output.save(\"/content/data/output2/output\"+str(i)+\".png\")\n","    '''\n","    psnr = psnr_between_rgb(sethrhr,setpreds)\n","    sum_psnr+=psnr\n","  print('PSNR: {:.2f}'.format(sum_psnr/14))\n"],"metadata":{"id":"l9ZRunMZni0Z"},"execution_count":null,"outputs":[]}]}